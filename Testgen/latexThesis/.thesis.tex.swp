 
\documentclass[12pt,oneside]{book}

\usepackage{fullpage}
\usepackage{amsmath}
%%\usepackage{amssymb}
%%\usepackage{amsfonts}
\usepackage{listings}
\usepackage{color}
\usepackage{graphicx}
\usepackage{soul}
\usepackage{subcaption}
%%\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{epsfig}
%%\usepackage{multirow}
\usepackage [english]{babel}
\usepackage [autostyle, english = american]{csquotes}
\usepackage{algorithm}
%%\PassOptionsToPackage{noend}{algpseudocode}
\usepackage{algpseudocode}
\usepackage{setspace}

\MakeOuterQuote{"}

\rfoot{Page \thepage}
\fancyhead{}
\rhead{\thepage} 
\lhead{} 

\hyphenation{thatshouldnot}

\geometry{
 a4paper,
 total={210mm,297mm},
 left=35mm,
 right=25mm,
 top=25mm,
 bottom=25mm,
}
\linespread {1.6}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=C,
  %%lineskip={-1.5pt}
  aboveskip=4mm,
  belowskip=7mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\ttfamily\footnotesize\singlespacing},
  numbers=none,
  numberstyle=\tiny\color{mauve},
  keywordstyle=\color{dkgreen},
  commentstyle=\color{blue},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

%%-------------------------------------------------ALGORITHM SETTINGS-----------------------------------------


\errorcontextlines\maxdimen

% begin vertical rule patch for algorithmicx (http://tex.stackexchange.com/questions/144840/vertical-loop-block-lines-in-algorithmicx-with-noend-option)
\makeatletter
% start with some helper code
% This is the vertical rule that is inserted
    \newcommand*{\algrule}[1][\algorithmicindent]{\makebox[#1][l]{\hspace*{.5em}\thealgruleextra\vrule height \thealgruleheight depth \thealgruledepth}}%
% its height and depth need to be adjustable
\newcommand*{\thealgruleextra}{}
\newcommand*{\thealgruleheight}{.75\baselineskip}
\newcommand*{\thealgruledepth}{.25\baselineskip}

\newcount\ALG@printindent@tempcnta
\def\ALG@printindent{%
    \ifnum \theALG@nested>0% is there anything to print
        \ifx\ALG@text\ALG@x@notext% is this an end group without any text?
            % do nothing
        \else
            \unskip
            \addvspace{-1pt}% FUDGE to make the rules line up
            % draw a rule for each indent level
            \ALG@printindent@tempcnta=1
            \loop
                \algrule[\csname ALG@ind@\the\ALG@printindent@tempcnta\endcsname]%
                \advance \ALG@printindent@tempcnta 1
            \ifnum \ALG@printindent@tempcnta<\numexpr\theALG@nested+1\relax% can't do <=, so add one to RHS and use < instead
            \repeat
        \fi
    \fi
    }%
\usepackage{etoolbox}
% the following line injects our new indent handling code in place of the default spacing
\patchcmd{\ALG@doentity}{\noindent\hskip\ALG@tlm}{\ALG@printindent}{}{\errmessage{failed to patch}}
\makeatother

% the required height and depth are set by measuring the content to be shown
% this means that the content is processed twice
\newbox\statebox
\newcommand{\myState}[1]{%
    \setbox\statebox=\vbox{#1}%
    \edef\thealgruleheight{\dimexpr \the\ht\statebox+1pt\relax}%
    \edef\thealgruledepth{\dimexpr \the\dp\statebox+1pt\relax}%
    \ifdim\thealgruleheight<.75\baselineskip
        \def\thealgruleheight{\dimexpr .75\baselineskip+1pt\relax}%
    \fi
    \ifdim\thealgruledepth<.25\baselineskip
        \def\thealgruledepth{\dimexpr .25\baselineskip+1pt\relax}%
    \fi
    %\showboxdepth=100
    %\showboxbreadth=100
    %\showbox\statebox
    \State #1%
    %\State \usebox\statebox
    %\State \unvbox\statebox
    %reset in case the next command is not wrapped in \myState
    \def\thealgruleheight{\dimexpr .75\baselineskip+1pt\relax}%
    \def\thealgruledepth{\dimexpr .25\baselineskip+1pt\relax}%
}
% end vertical rule patch for algorithmicx

%%------------------------------------------ALGORITHM SETTINGS END------------------------------------------------






\date{}
\begin{document}

%% ----------------------------------------------PAGE 1 TITLE

\begin{titlepage}
    \begin{center}
        \vspace*{1cm}
        
        {\scshape {\huge{I}\Large{NTER-PROCEDURAL} \huge{A}\Large{NALYSIS OF}\\
                   \vspace{0.1cm}
				   \huge{C}\Large{ONCOLIC } \huge{E}\Large{XECUTION}}}\\[1.5cm] % Thesis title

        
        \textit{A Thesis Submitted\\
        in Partial Fulfilment of the Requirements\\
        for the Degree of\\}
                \vspace{0.8cm}
        \textbf{\large{MASTER OF TECHNOLOGY}}\\[0.5\baselineskip]
        \textit{by}\\[0.5\baselineskip]
        \textbf{\large{Ashwini Kshitij}}\\
                \vspace{0.8cm}
        \textit{supervised by}\\
                \vspace{0.8cm}
        \textbf{\large{Dr. Subhajit Roy}}\\
        \textbf{\large{Dr. Amey Karkare}}
        \vspace{0.8cm}
        
      	\includegraphics[width=0.25\textwidth]{iitklogo.png}
        
        \vspace{1\baselineskip}
        
        \uppercase{Department of Computer Science and Engineering\\
        \textbf{Indian Institute of Technology Kanpur}}\\
        
        \vspace{1\baselineskip}
        \textbf{June 2015}
        
    \end{center}
\end{titlepage}


%%-----------------------------------------------PAGE 2 Certificate

\frontmatter
\addcontentsline{toc}{chapter}{Certificate}
\begin{center}
\textbf{\Large{CERTIFICATE}}
\end{center}
\vspace*{3\baselineskip}
This is to certify that the work contained in this thesis entitled \textbf{\textit{"Inter-procedural analysis of Concolic Execution"}}, by \textbf{Ashwini Kshitij (Roll No. 10327165)}, has been carried out under my supervision and this work has not been submitted elsewhere for a degree.\\
\\[4\baselineskip]

  \small
  \vspace{2.0 cm}
  \begin{tabular*}{1.0\textwidth}{@{\extracolsep{\fill}} l r}
    \textbf{Dr. Subhajit Roy} 			 & 				\textbf{Dr. Amey Karkare}\\
    Assistant Professor,				 &				Assistant Professor,\\
    Department of CSE, 					 & 				Department of CSE,\\
    IIT Kanpur.							 &				IIT Kanpur.
    
  \end{tabular*}
\newpage


%%----------------------------------------------ABSTRACT
\addcontentsline{toc}{chapter}{Abstract}
\begin{center}
\textbf{\Large{ABSTRACT}}
\end{center}
\vspace {0.8 cm}

Interprocedural analysis is the cornerstone of determining precise program behavioral information. Using this technique we can avoid making overly conversative assumptions about the effects of procedures and the state at call sites. It aims at gathering informations across multiple procedures.  

\vspace {0.5 cm}

In this thesis, to extend the concept of concolic execution to interprocedural calls we instrument the function call site. For such instrumentation purposes we have used a tool called CIL. Just before the function call we set up the environment for calling the procedure which in turn enables it to follow through the concolic execution of parameters with the control flow from caller to callee. Similarly, just after the call instruction we restore the calling environment by restoring the symbolic values.

\vspace {0.5 cm}

Modifying the symbolic execution engine to collect the interprocedural analysis information can have widepread applications in software verification and testing. We can improve the coverage of test suites that are automatically generated by intraprocedural concolic executers.   




\newpage

%%----------------------------------------------ACKNOWLEDGMENT
\addcontentsline{toc}{chapter}{Acknowledgement}
\begin{center}
\textbf{\Large{Acknowledgements}}
\end{center}

\vspace{1.0 cm}

I acknowledge, with gratitude, my debt of thanks to Professor Subhajit Roy for his advise and encouragement and to Professor Amey Karkare for his aid and foresight. They presented me with the opportunity to tackle interesting problems in field of Software Testing. Their patient but firm guidance was critical to successful completion of my research.  

\vspace{0.5 cm}

I appreciate the support of my friends and wingmates who always provided me with the confidence and courage to tackle even the most challenging problems. Without their help and councel, the completion this work would have been immeasurably more difficult. 

\vspace{1.0 cm}

I also want to express my sincere gratitude to \textit {\textbf {BRNS}} for encouraging our research work. Their encouragement motivated me to follow through this project.

\begin{flushright}
\textbf{-Ashwini Kshitij}
\end{flushright}

\newpage

%%----------------------------------------------CONTENTS

\tableofcontents
\listoffigures
\listoftables


\mainmatter

%%---------------------------------------------CHAP 1 Intro

\chapter{Introduction}
%%\lhead{Chapter 1. \emph{Introduction}}


\hspace {0.4 cm}
Symbolic execution has been a recipient of significant attention during the past few years. It is now considered an effective technique in generation of high coverage test suites. The idea has been discovered around three decades ago but it was after significant improvements the potential of the idea was harnessed. One such important improvement was symbolic execution alongside keeping track of concrete values (concolic execution). The main advantage of this technique is that whenever constraint solving complications (like timeouts) occur during classical symbolic execution, it is alleviated using the concrete values.

All practical programs involve procedure calls. The symbolic execution is relatively simple if there are no function calls involved (\textit{intraprocedural analysis}). A function call tranfers the control from the caller to the callee. That function may very well modify the symbolic state of the variables. If we transition the symbolic state correctly through the function call, the symbolic execution will run correctly with the modified symbolic state and collect the precise alteration made to symbolic state within the procedure.    

%%----------------------------------------------MOTIVATION

\section{Motivation}
\textit{Intraprocedural Analysis} is performed on one procedure at a time. It is simple and conservative. But almost all real world programs involve procedural calls. This poses a problem for techniques such symbolic execution since they dont know how the procedure is going to modify the symbolic state. This brings us to the need of extending our analysis techniques to prgrams with functions and procedures.\\

\begin{lstlisting}[caption=Motivational example]

			1		#include <stdio.h>
			2
			3		int dbl(int x) {
			4 			return 2*x;
			5		}
			6
			7		int main(void) {
			8 			int a;
			9			scanf("%d",&a);
			10 		if (dbl(a) + 3 == 9) {
			11	   		return 0;
			12 		} else {
			13	   		return 1;
			14 		}
			15		 }

\end{lstlisting}

\textit{Listing 1.1} is an example program from CREST tool. In this program \textbf{a} is treated as an input. When intraprocedural concolic testing is done on \texttt{main()} procedure of this program only fifty percent coverage is generated. This implies only one branch is covered in this program. The first test case is a randomly generated value of \textbf{a} that covers one branch irrespective of its value. In the next iteration, the program is not able to generate an input that covers the other branch to produce 100\% coverage. The next test case is generated using condition \texttt{dbl(a) + 3 == 9} in its path constraint, which is equivalent to condition \texttt{2a + 3 == 9}. But intraprocedural analyser fails to infer that \texttt{dbl(a) = 2a}. Thus a need arises to implement support for interprocedural analysis in concolic testing.      

%%----------------------------------------------PROBLEM STATEMENT

\section{Problem Statement}
This thesis aims to provide a method that can capture the transformation of the symbolic state of the inputs when the program goes through a function call. Our implementation uses inbuilt concolic execution engine for the purpose. But the engine cannot handle a procedure call. At the call site, the symbolic value is reset to "start state" instead of modifying it as per the operations performed on it by the callee function. Merging our approach with this concolic engine will increase the precision of test case generation. 

\begin{figure}[htbp]
\centering
\includegraphics[scale=1]{module3.png}
\caption{Basic Module}
\end{figure}

\textit{Figure 1.1} shown above gives a high-level idea of what our implementation does. We are given a C program with a concolic execution engine integrated with it. Problem arises with execution when it comes across a procedure call instruction. The symbolic state of the input variables is set to default as the program does not have the ability to follow through symbolic execution from caller to callee and then back to the caller.

We aim at solving this problem by instrumenting the call site. For such purposes in our approach we have used a tool called CIL. We first set up an environment before commencing the callee procedure execution (in this case \textit{foo}). The operations for such tasks is carried out by \textit{funcEntry} function which is instrumented just before \textit{foo} is called. \textit{funcEntry} handles the mapping of symbolic and concolic values of actual parameters to formal parameters, the creation of symbolic stack (later explained in \textit{Chapter 3}).

Similarly we instrument \textit{funcExit} function just after \textit{foo} call. The task of \textit{funcExit} is to map the symbolic state of "return variable" to appropriate variable at the call site (if any) and clear the symbolic values that were generated during call and execution of \textit{foo}. It also manages the symbolic stack appropriately. The implementation of \textit{funcEntry} and \textit{funcExit} is done in C language and is defined in included C modules.

This approach ensures that symbolic execution is carried through correctly to and from the callee. The modified state of symbolic variables is in accordance with operations of concerned callee function (in our case \textit{foo}). This improves the accuracy and correctness of the test suite generation in our tool.  

%%----------------------------------------------CONTRIBUTIONS

\section {Contributions}
In this thesis we have accomplished the following:
\begin{itemize}
  \item Designed an approach to apply inter-procedural analysis to concolic execution. 
  \item Unit tests for verifying the features and functionality of test case generation tools. 
  \item Dynamic support to handle recursive procedure calls. 
\end{itemize}

%%----------------------------------------------ORGANISATION OF THESIS

\section {Organisation of thesis}
A brief summary of the contents of other chapters of this thesis are as follows.\\

\textbf{Chapter 2} deals with the concepts which are necessary to understand the thesis. Concolic execution is introduced and its advantages over classical symbolic execution and random testing. An overview of approaches in interprocedural analysis like Function Inlining and Call String have also been discussed in this chapter. We have also briefly mentioned the related work that has been done in this field. \\

\textbf{Chapter 3} discusses our implementation in a detailed manner. Some terminology has been introduced to explain techniques employed in our work. But mostly the chapter pertains to the relevant algorithms designed to accomplish the aim of this thesis and how it should improve the scalability and accuracy of concolic IP analysis.\\

\textbf{Chapter 4} contains the experimental results and their analysis after we run our modified tool on a set of standard benchmark programs. Analysis in done in terms of coverage improvement and runtime of the tool.\\

\textbf{Chapter 5} sheds light on future work and modifications that can be done to improve the accuracy and performance of the approach discussed in this thesis.

%%----------------------------------------------CHAPTER 2 : BACKGROUND

\newpage
\chapter{Background}

%%----------------------------------------------INTERPROCEDURAL DATAFLOW ANALYSIS

\section {Interprocedural Data-flow Analysis}
This is a technique which is broadly defined as gathering of information across multiple procedures (typically over the entire program). Procedure call poses barrier to program analysis. Its aim is to avoid making conservative assumpions about the effect of procedures and the state at call sites.

Interprocedural analysis is more demanding and challenging than intraprocedural analysis. Here we need to take into account the call-return and parameter passing mechanisms, local variable of the function and function recursion (can be unbounded). In some cases the called procedure is known only in run-time like with function pointers or with virtual functions. 

\textbf{Application :} Interprocedural analysis (IPA) enables the compiler to optimize the code across different files (whole-program analysis), and can result in significant performance improvements of the program by removing spurious data dependencies. Integrating IPA with intraprocedural concolic execution will help compute the precise behavior of function calls on the symbolic state of the variables.

%%----------------------------------------------FUNCTION INLINING 

\subsection {Function Inlining}
One of the approaches is \textit{function inlining} which is the simplest and most widely used approach for accurate interprocedural symbolic execution. We simply use the copy of procedure's Control Flow Graph (CFG) at each call site. This leads to function being re-analyzed at every call site. This is can be avoided using \textit{function summaries}. They are implemented by merging all states at the function exit after computing an intraprocedural path constraint in terms of function input.\\ 

\begin{figure}[htbp]
\centering
\includegraphics[scale=0.5]{inlineCFG.png}
\caption{Function Inlined}
\end{figure}

This technique does not require function calling overhead. It also saves the overhead for manipulating function stack. In increases locality of reference by utilizing instruction cache.

But there are significant \textit{drawbacks} with this approach. Firstly, the performance overhead will increase if we increase the size of the code that is to be inlined. The caller function may not fit on the cache causing high cache miss rate. Similarly if there are too many function calls involved, inlining maybe expensive since it will cause an exponential increase in the size of the CFG.

Secondly it is also going to create problems in the recursive procedures. Same can be said more generally for any scenario where there are cycles in the call graph. So basically function inlining has scalability issues pertaining to code size. Lastly, procedure inlining is only possible if the target of the call is known. Hence it will not be possible if call is via a pointer or is "virtual".

%%----------------------------------------------CALL STRING APPROACH

\subsection {Call String Approach}
In another IP analysis we observe the CFG's of all the procedures. In this technique,
\begin{itemize}
\item When we encouter a function call we interpret it as a goto from the call intruction to the first instruction of the procedure.
\item Interpreting every return statement like a goto to the instruction following each call site that invoked that procedure (\textit{Non-Deterministic}).
\end{itemize}
Using this method non-deterministically will allow non feasible paths in our analysis which will cause loss of accuracy. In a better approach called "Call String" we keep track of the where we came from, that is, the context of the call and where to return. To do this we maintain a "string" that simulates a call stack. A feasible path is a control flow path that is generated in accordance with the stack regime. A perfect solution is keep record of the whole stack. This concept is easy and intuitive according to which every return jumps to the instruction that's immediately after the call site which corresponds to that particular function call. By adding the context of the call to the information in the state we can overcome the problem of passing through invalid paths.\\

\begin{figure}[htbp]
\centering
\includegraphics[scale=0.6]{callString.png}
\caption{Example : Call String}
\end{figure}  
 
Every procedure records their state information when they are invoked. Let us consider the program in \textit{Figure 2.2}. Here the labels \textit{c1} and \textit{c2} are saved in the state information of \textit{foo}. This is so that \textit{p} gets different values every time function \textit{foo} is called. Another reason to save the labels is because they help in figuring out the respective return values of call \textit{c1} (\$\$ $\rightarrow$ 4) and \textit{c2}(\$\$ $\rightarrow$ 6).

Implementing this algorithm is efficient for small strings. Problem arises when call string generated is large since we can keep track of only limited number of strings. This poses a limitation on the depth of function calls.

%%----------------------------------------------CONCOLIC TESTING


\section {Concolic Testing}

One popular approach for automated software testing is \textit{random testing} which involves subjecting the program to be tested to a stream of random data. It is fast but it can find only basic bugs like program crashes, code assertions or memory leaks. But it is not always possible to employ this technique specially when working with binaries since it is very difficult to figure out the expected inputs. But random Testing fails to cover corner cases.\\

A more deliberate approach can be used by merging symbolic execution with concrete execution (\textit{concolic execution}) and then using a an SMT-solver (like Z3) to generate test inputs. This method is called \textit{Concolic testing}. It is much more efficient than symbolic execution. To understand why let us assume that our costraint solver cannot handle non-linear constraint. Then a path constraint with some \textit{non-linear} function involved cannot be solved and symbolic execution will be stuck. Similar problem arises when we encounter a closed third party library function say \textit{increment()}. Symbolic execution alogorithm doesn't know how to modify the symbolic state according to the behavior of that function. For such cases the test cases cannot be generated by classical symbolic execution. Concolic Tesing addresses these limitations and resolves situations like above by replacing the symbolic values by their concrete values so that the resulting constraint can be solved by the constraint solvers.\\

The algorithm involves initialting the inputs with randomly generated values. The program is executed and during the execution, on every conditional branch statement program collects symbolic path constraints on inputs. Symbolic constriants are essentially a set of logical constriants on input data. New program path is directed and executed by \textit{negating/flipping} the last condition of the path constraint. This is done until all the feasible program paths are explored. To get the intuition, let us consider the program in \textit{Figure 2.2} for concolic testing.  

\newpage

\begin{figure}[htbp]
\centering
\includegraphics[scale=0.5]{conjoined4.png}
\caption{Concolic Testing}
\end{figure}

The program input variables are assigned random concrete values, say \textit{x = 1} and \textit{y = 1}. The program runs and line 11 is executed. Path condition generated is :

\begin{equation}
\neg(y == 3x)
\end{equation}

Now we negate the last condition in path constraint (PC has only one condition in this case) to execute an alternate path. So the new path constraint generated is :

\begin{equation}
y == 3x
\end{equation}

which is examined by a SMT solver to generate the new input data. Out of many different possible values for equation \textit{2.2} solver picks say \textit{x = 1} and \textit{y = 3} which should explore a different path than before. The program runs on these new inputs and generates a new path condition :

\begin{equation}
(y == 3x) \wedge \neg(y == x + 10)
\end{equation}

Like previously we negate the last condition of the path constraint to generate condition for new path exploration.
So we pass on the following constraint to solver to generate new inputs

\begin{equation}
(y == 3x) \wedge (y == x + 10)
\end{equation}

Let the new inputs generated be \textit{x = 5} and \textit{y = 15} we will lead to the execution of line 13 and consequently line 14 which will hit the error or any other unexpected behavior. Since there are no more paths left to explore, the algorithm will terminate generating a set of input data for complete path coverage of this program.

%%----------------------------------------------RELATED WORK

\section {Related work}

%%----------------------------------------------SUMMARY BASED ANALYSIS

\subsection {Summary-based Analysis}
New techniques have been invented that compute procedure summaries for performing an interprocedural analysis of programs. In summary-based context-sensitive analysis we create "summary" which is succinct description of the observable behavior of each procedure. The purpose of this approach is to prevent renanalysing the behavior of same procedure when there are invoked at each call site.

The representation of every procedure has a single entry point. The analysis is divided into following two phases:
\begin{itemize}
  \item In fist phase we summarize the effects of a procedure and a transfer function is computed in a bottom-up manner.
  \item  In second phase we propogate the caller information to compute callee result in a top-down manner.
\end{itemize}

%%----------------------------------------------DART

\subsection {DART}
DART is an abbreviation for "\textit{Directed Automated Random Testing}" which is a tool for automated test case generation developed by \textit{Patrice Godefroid, Nils Klarlund} and \textit{Koushik Sen}. It utilizes the concept of concolic execution and is comprised of the following techniques :
\begin{itemize}
 \item \textit{Automated} extraction of program interface from source code.
 \item \textit{Random testing} the program interface by generating a test driver. 
 \item Dynamic generation of test cases to \textit{direct} alternate program execution path.
\end{itemize}

\newpage

%%----------------------------------------------CUTE

\subsection{CUTE}
CUTE stands for "A Concolic Unit Testing Engine for C" which also addresses automatic test case generation with memory graphs as inputs. This tool is developed by \textit{Koushik Sen}, \textit{Darko Marinov} and \textit{Gul Agha}. It is similar to DART, thereby employs concolic testing technique. It resolves some of the limitations of DART and aims at testing real-world examples of C code.

It provides a method for representing and solving approximate pointer contraints to generate test inputs. The symbolic model being used is more powerful and the theorm solver is both more powerful and is built to be efficient in this system. As opposed to what DART does, CUTE does not automatically extract program interface but lets user decide relation among functions and their preconditions. The work also shows exactly how it made approximations and trade off between speed vs. correctness and scenarios where CUTE will not work correctly.






\newpage


\chapter{Methodology}
The algorithm that has been used to implement the interprocedural analysis in the concolic execution engine has been described in detail in this chapter.

\section{Definitions}

We are given a program \textbf{P} which has a concolic execution engine \textbf{C} integrated with it. Our aim is to combine interprocedural analysis with \textbf{C} so that the test case generation can also take into account the effect of function calls. To implement this algorithm we will instrument the code of P with our auxiliary code which will not alter the outcome of the program \textbf{P}. Let 

\begin{itemize}
 \item \textit{I} be the input generated by the tool for program P.
  \item \textit{\textbf{f}} be the callee function used in call site.
 \item \textit{Output(P,I)} be the output of the program P when run on input I. 
 \item \textit{\textbf{S}} denote Variable Stack explained in \textit{Section 3.3.3}.  
\end{itemize}

\section{Static Analyser}
We have used a tool called \textbf{CIL} (\textbf{C} \textbf{I}ntermediate \textbf{L}anguage) to perform static analysis of program P and source-to-source transformations on it. We do this by traversing the AST (Abstract Syntax Tree) which is the in-memory data-structure which represents the parsed program P.

\subsection{Handle Expressions}

This module accomplishes two tasks. It modifies the call site by simplifying the arguments passed to the calle function. It also simplifies the return expression in the callee function. At the call site the arguments of the function are examined one by one. If an argument is not a variable or literal then it must be an expression. In case of expression, we store the argument in a temporary local variable and pass that new variable as the parameter instead. Similar approach applies to return values. If the return statement consists of a variable, it is left unchanged otherwise it is transformed.\\

\begin{lstlisting}[caption=Expressions simplified]

  int func(int x,int y){								int func(int x,int y){
  {																int tmp;
    if (y < = 1 )											   int cil_tmp4, cil_tmp4, cil_tmp4;
       return x;												if (y < = 1 )
    else														   {
       return x * func(x-1,y);							    cil_tmp4 = x;
  }																    return cil_tmp4;
     															    } else {
																       cil_tmp5 = x - 1;
																       tmp = func(cil_tmp5, y);
																       cil_tmp6 = x * tmp;
																       return cil_tmp6;
																    }
															    }

\end{lstlisting}


We can see in \textit{figure 3.1} the call sites and return statements have been transformed such that in further stages we only have to deal with variables and literals. We will not have to deal with complicated expressions. To create new local temporary variable we use CIL API. It ensures to generate unique variable names to avoid any ambiguity.
This phase will have no effect on function calls with no arguments or functions which don't return any value (void).
\newpage

\subsection{Variable Renaming}
The concolic testing engine used has symbolic table structure that is globally defined. The entries in the table are manipulated using variable names of program P as the key. In intraprocedural symbolic execution duplicate variable names are not an issue.

But when dealing with multiple procedures all of which are capable of executing symbolically, two different procedures may very well have variables with the same name. When these variables are used as the key to manipulate the values in symbol table, it leads to undefined behavior. Therefore we need to make the variables of all the procedures unique. This is done by merging the variable name with the name of the procedure is whose scope the variable belongs. \textit{Figure 3.1} demonstrates the renaming of the variables to resolve the conflict of symbol table keys. \\

\begin{figure}[htbp]
\centering
\includegraphics[scale=0.45]{renamed1.png}
\caption{Variable Renaming}
\end{figure}  

This method will work for non-recursive procedure calls. To handle recursion we have developed an extended version of approach which will be discussed in \textit{section 3.4}.\\

%%\newpage

\subsection{Extend Concolic Execution Engine}
The current engine uses approaches such as handling set (or assignment) instructions symbolically and unrolling of loops to carry out the symbolic execution. But these aproaches are only applied on the procedure that the user demands to be tested. The remaining procedures are unchanged.\\
Now on function call we want the symbolic execution to be able to continue in the callee \textit{f}. Hence we need to instrument all the procedures of program P such that it enables them to be executed symbolically. So instead of applying the loop unrolling and symbolic assignment only on user input procedure, they are applied on every procedure.

%%This can be understood using \textit{figure 3.3} which shows a function \textit{foo} which is not the function to be tested. Originally, \textit{foo} is unchanged since it plays no role in symbolic execution of program. Since we have want to extend the concolic engine to be executed interprocedurally, it is modified as shown. 

\subsection{Call Site Transformation}
Without interprocedural call handling whenever the concolic execution encountered a function call, it used the concrete value of the function result. There interprocedural symbolic analysis could not be done. This problem is addressed by setting up a function call environment and function return environment, that will facilitate the transfer of symbolic state from caller to callee and back. The pseudo-code of the algorithm is mentioned ahead.\\


\begin{algorithm}[H]
  \caption{Transform Call Site}\label{transform call site}
  \small
  \vspace{4mm}
  {\textbf{Input} : Program P}\\
  {\textbf{Output} : P with modified call sites}
  \begin{algorithmic}[1]
    \Procedure{transform}{}%%\Comment{The g.c.d.\ of a and b}%%
      \vspace{0.8mm}
      \For{\texttt{each procedure f $\in$ P}}
        \vspace{0.8mm}
        \State \texttt{$instList \gets $instructionList(f)}
        \vspace{0.8mm}
        \For{\texttt{each procedure f $\in$ P}}
          \vspace{0.8mm}
          \If {$I'$  is a call instruction $C$}
            \myState {$argsList$ $\gets$ getArgumentsAsString($C$.$fname$)}
            \myState {$localsList$ $\gets$ getLocalsAsString($C$.$fname$)}
			\myState {$funcEntryInst$ $\gets$ makeCallInstruction($C$.$fname$)}
			\myState {$funcExitInst$ $\gets$ makeCallInstruction( )}
			\myState {$instList$ $\gets$ $instList$ :: [funcEntryInst :: I' :: funcExitInst]}
          \EndIf
        \EndFor
      \EndFor
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\textbf{Algorithm 1 Description} : The input to this algorithm is program P. We inspect all the procedures of P and detect the call sites. It is at these call sites where we need to do transformations such that when control flows from caller to callee, the callee has the symbolic information needed to execute symbolically.\\

One such call site be \textit{c} which invokes a function \textit{f}. Parameters that are passed to the function \textit{f} have certain symbolic and concrete values associated with them. Before \textit{f} starts executing, the mapping of concolic values needs to be done from actual parameters to formal parameters. For that we have to analyse the formal and actual parameters, and figure out a way to send their combined information to \textit{funcEntry}, which does the actual task of concolic mapping.\\ 
The function \textit{getArgumentsAsString} in \textit{line 6} has two parts. Firstly, it extracts the information about actual parameters of \textit{f} like if its a literal or variable (its name). Secondly, it analyses the formal parameters of \textit{f}'s prototype and gathers information about their name and type. Then this information is merged into a string, one parameter at a time.\\

This string is passed as a paramter to \textit{funcEntry} along with another string containing the names of local variables (created by \textit{getLocalsAsString()} in \textit{line 7}). The function instrumented after call site \textit{c} is \textit{funcExit} that does the task of cleaning up the intermediate data-structures created by \textit{funcEntry} and were necessary to execute \textit{f} symbolically.\\



\vspace{0.4cm}

\begin{figure}[htbp]
\centering
\includegraphics[scale=0.53]{env1_orig.png}
\caption{Original Code}
\end{figure}  

In the example shown in \textit{Figure 3.2} there is a call instruction at \textit{line 4} invoking function \textit{fact} with two paramters. The information about both actual and formal paramters in analysed and sent to \textit{funcEntry} as string argument at \textit{line 12} in \textit{Figure 3.3}. The second argument to \textit{funcEntry} is names of local variables of function \textit{fact}. Using these arguments, environment for \textit{fact} to execute symbolically is set up by \textit{funcEntry}. Similarly, \textit{funcExit} at \textit{line 15 in Figure 3.3} handles the task of cleanup of this environment after \textit{fact} has finished executing.

\vspace{0.4cm}
\begin{figure}[htbp]
\centering
\includegraphics[scale=0.56]{env2_modified.png}
\caption{Transformed Call Site}
\end{figure}  

At \textit{line 16} in \textit{Figure 3.3} we also have function \textit{add\_entryToSTable} which is used to map the returned concolic values of function \textit{fact} to appropriate variable, which is in our case \textit{a}. This will be discussed in \textit{Section 3.3.4}. 

\newpage

\section{Set up Environment For Procedure Call}
This section involves the runtime analysis of the arguments that are passed to \textit{funcEntry}. We will explain their utilization to create entries in global symbol table, to enable callee function \textit{f} to execute symbolically.

\subsection{Organising Argument Details}
Using the information of actual and formals parameters passed to the \textit{funcEntry}, we construct a data structure \textit{Argument} that has the following attributes: 
\begin{itemize}
\setlength\itemsep{0em}
 \item \textit{funcName} : name of the function to which the argument is passed.
 \item \textit{vname} : name of the associated formal parameter.
  \item \textit{type} : 1 for int and 2 for real.
 \item \textit{apname} : if actual argument is a variable, then its name.
  \item \textit{val} : if actual argument is a literal, then its value. 
\end{itemize}
Note that actual parameter can either be a variable or a literal, implying that only one among \textit{apname} or \textit{val} can have a valid value. The other is going to be \textit{null} so we have to use them accordingly. We have passed this parameter information along with local variable names of \textit{f} to \textit{funcEntry} as discussed in \textit{Section 3.2.4}. Using string tokenizing and parsing within \textit{funcEntry}, we get a list of \textit{Arguments} type structures.\\
This step is demonstrated in \textit{Figure 3.4} in reference to program shown in \textit{Figure 3.3}.

\vspace{0.4cm}
\begin{figure}[htbp]
\centering
\includegraphics[scale=0.60]{arguments1.png}
\caption{Two arguments at \textit{line 12} in \textit{figure 3.3}}
\end{figure}

%%\newpage

\subsection{Populate Symbol Table}
After we created a systematic model of arguments, we need to populate the global symbol table accordingly. Besides the arguments, we also need to handle local variable in \textit{f}. For locals, we create empty entries in symbol table. The algorithm used in this implementation is described below\\


\begin{algorithm}[H]
  \caption{Populate Symbol Table}\label{transform call site}
  \small
  \vspace{4mm}
  {\textbf{Input} : Argument list A, locals list L, symbol table S}\\
  {\textbf{Output} : Modified symbol table S' with entries for A and L}
  \begin{algorithmic}[1]
    \Procedure{populateStable}{$A,L,S$}%%\Comment{The g.c.d.\ of a and b}%%
      \vspace{0.8mm}
      \For{\texttt{each argument $a$ $\in$ A}}
        \vspace{0.8mm}
          \If {$a.apname$  is a literal}
            \myState {$sym$ $\gets$ {\texttt{Constant}}}
            \myState {$con$ $\gets$ $a.val$}             			 \Comment{$a.val$ is the value of $a$}
          \Else
            \myState {$sym$ $\gets$ findSymbolicValue($a$.$apname$)} \Comment{a.apname is actual parameter }
			\myState {$con$ $\gets$ findConcreteValue($a$.$apname$)} 
          \EndIf
			\myState {addEntryToSTable($a$.$vname$, $sym$, $con$)}
          \vspace{0.8mm}
      \EndFor
      \For{\texttt{each local variable $l$ $\in$ $L$}}
        \vspace{0.8mm}
	    \myState {createEmptyEntryInSTable($l$.$name$)}
      \vspace{0.8mm}
      \EndFor
    \EndProcedure
  \end{algorithmic}
\end{algorithm}



\vspace{6mm}
\noindent
\textbf{Algorithm 2 description} : This algorithm maps the symbolic and concrete values of actuals parameters at call site to the formal parameters of the function \textit{f}. This is done before \textit{f} starts executing. For local variables, initially empty entries are created in the symbol table. Functions \textit{findSymbolicValue} and \textit{findConcreteValue} at \textit{line 8 and 9} search the symbolic and concrete values for a particular variable in the table. They are defined in the C modules of concolic testing engine.

\subsection{Variable Stack}
In previous section, before the execution of \textit{f} we have to populate symbol table with respective variables of \textit{f}. But it is important to do systematic clean up after \textit{f} is done executing. This includes deletion of entries from the table that were temporarily required for symbolic execution of \textit{f}. It is required so that spurious values may not only cause inconsistencies but also caused the symbol table to overflow. 
For this purpose, we maintain a stack S called \textit{variable stack}. Each element of the stack stores the information regarding variable entries in the symbol table for indiviual function. The contents of stack element are as follows : 
\begin{itemize}
\setlength\itemsep{0em}
 \item \textit{funcName} : name of the function for which this element is created.
 \item \textit{args} : string array containing the names of formal paramters of \textit{funcName}.
  \item \textit{locals} : string array containing names of local variable in the scope of \textit{funcName}.
  \item \textit{occurence} : used to indicate the instance of \textit{funcName} in the call stack. This is explained in detail and how it is used to resolve recursive calls in \textit{Section 3.4.2}. 
\end{itemize}

\vspace{0.3cm}
\begin{figure}[htbp]
\centering
\includegraphics[scale=0.50]{stack.png}
\caption{Variable Stack}
\end{figure}

The behavior of variable stack is similar to function call stack. Whenever a function call is ecountered the variables (both arguments and locals) of the callee function are pushed onto the stack S.. For example, in \textit{Figure 3.5} when \textit{f} is called in \textit{main}, its variable details are pushed onto S. At the time statement S2 executes in \textit{g}, stack has three elements as shown in the figure.
When we return from a function call, we pop an element from the stack and delete the entries in the symbol table by referring the popped element.\\
The task of pushing the element on encountering a function call is done by a method \textit{funcEntry} and popping the element after returning from that function is done by \textit{funcExit}, both of which are defined in C modules.

\subsection{Return Handling}
After the environment is set up for \textit{f}, it executes symbolically covering a specific path in the control flow graph of \textit{f}. It collects a summary that tranforms the symbolic values of parameters to new symbolic return values. We need to map the concolic return values to appropriate variable at call site.\\ 
For this, just before the function \textit{f} returns, we copy the concolic return value to a global variable. Then at call site, we assign the lvalue of call instruction to this global concolic variable. Thus, the concolic execution flows to and from the callee correctly.   

\vspace{0.3cm}
\begin{figure}[htbp]
\centering
\includegraphics[scale=0.40]{return.png}
\caption{Concolic Return}
\end{figure}

Here in \textit{Figure 3.6}, function \textit{f} is called with parameter x. Its symbolic and concrete value is modified according to function f', which is a transition function along that path in \textit{f}. Variable \textit{z} attains this new concolic value and is returned. To ensure transfer of modified symbolic information on function return, we map the concolic value of \textit{z} to global variable and from them to \textit{t} in \textit{main}. Note that even if a function does not return a value it may alter the global values.

%%\newpage

\section{Recursion}
\subsection{Limitation of previous approach}
The approach that we have discussed until now can only handle non-recursive procedure calls. The reason being that our algorithm did not have support for multiple versions of same variable of the same function in the symbolic table. Our variable stack can handle atmost one instance of a function at any given time.
In \textit{Section 3.2.2} we have already resolved the issue of same named variables in different procedures.\\
To understand this, let us consider an example of recursive code.

\vspace{0.4cm}
\begin{figure}[htbp]
\centering
\includegraphics[scale=0.60]{fac.png}
\caption{WCET program for factorial}
\end{figure}

In this example, at \textit{line 14} a function call has been made to \textit{fac}. Before the call instruction executes, our algorithm maps concolic values from \textit{x} to \textit{fac\_n} and creates an entry in the symbol table using \textit{fac\_n} as the key. Now when the \textit{fac} executes, it again encounters a call instruction at \textit{line 7}. Our algorithm should map the values from \textit{fac\_n - 1} to "new" \textit{fac\_n}. But it has no mechanism implemented to distinguish between different versions of same variable of the same procedure. This leads to overwriting of previously present entry of \textit{fac\_n} in the symbol table (refer to \textit{table 3.1}) . More important problem that will arise is that after \textit{fact} at \textit{line 7} executes and returns, it cannot restore the previous symbolic state (i.e. before call was made). The concolic values of previous versions of the recurring function in the stack will be lost. \\
Therefore, we need to modify the key of symbol table such that we can keep track of multiple instances of same function in the variable stack, allowing recursive call analysis.



\begin{table}	
\begin{subtable}{.5\linewidth}
\centering
\begin{tabular}{||c c c||} 
 \hline
 \textbf{key} & \textbf{symVal} & \textbf{conVal} \\ [0.5ex] 
 \hline\hline
  & : &   \\ 
 %%\hline
  & : &  \\
 \hline
 fac\_n & s0 & c0 \\
 \hline
  & : &  \\ [1ex] 
 \hline
\end{tabular}
\caption{call at line 14}
\end{subtable}%
\begin{subtable}{.5\linewidth}
\centering
 \begin{tabular}{||c c c||} 
 \hline
 \textbf{key} & \textbf{symVal} & \textbf{conVal} \\ [0.5ex] 
 \hline\hline
   & : &   \\ 
  \hline
  fac\_n & s1 & c1 \\
  %%\hline
  \st{fac\_n} & \st{s0} & \st{c0} \\
  \hline
  & : &  \\ [1ex] 
 \hline
\end{tabular}
\caption{call at line 7}
\end{subtable}
\caption{Symbol Table} 
\end{table}

\subsection{Need for Versions}
We have to maintain versions of function variables according to number of instances they have in the variable stack. We have to make sure that the entries in the symbol table dont get deleted or overwritten unless they are not needed anymore. One way is to modify the keys (of symbol table) such that they have function version associated with them. \\
Let \textbf{occurence} of a procedure at any time be defined as the number of instances of that same procedure present in the call stack.\\

\vspace{0.4cm}
\begin{figure}[htbp]
\centering
\includegraphics[scale=0.50]{symtableandstack.png}
\caption{Function Variable Versions in Symbol Table}
\end{figure}
Therefore we remodify the key defined in \textit{Section 3.2.2} to keep track its associated variable, procedure to which that variable belongs and that procedure's version (\textit{occurence}) it the call stack. The new key is:

\begin{equation}
K' = f'(V,P,O_{\tiny{P}})
\end{equation}

where O$_{\tiny{P}}$ is the occurence of P in function call stack.
In \textit{Figure 3.9} we can see the version handling of multiple instances of same functions in the symbol table. When another instance of function is pushed in the stack (recursively), no value in the symbol table is overwritten. Similarly, when a function instance is popped from the stack, the concolic values of previous version should be accessed.

\subsection{Variable Hash-Map}
The issue that arises with this approach is that we cannot rename variables in conjunction with their versions (like we did in \textit{Section 3.2.2} using static analysis). This is because the versions are created dynamically at runtime when function call stack is manipulated. Therefore we create a hashmap \textit{M} that maps the variable name to key in the symbol table. At any point during execution, if procedure P is in the call stack, there will exist a mapping such that if K = f(V,P) then \textit{M} has following \textit{(key, value)} pairs

\begin{equation}
\textit{M} = (K,K')
\end{equation}

Whenever concolic execution needs to refer the symbol table using variable V, we look up the \textit{value} corresponding to V in \textit{M} and return the \textit{key}. But we still have to maintain the hashmap so that it always give the correct version of the key (the state of the variable stack is dynamic and so are the versions of function variables). To explain this let us consider a call instruction in the program that invokes function \textit{f}. The operations before and after the execution of \textit{f} are as follows.

\subsubsection{Before Function Execution}
In \textit{Section 3.2.2} we discussed the algorithm to populate the symbol table with specific values before executing a call instruction. We modify that algorithm slightly, instead of using variable name K as key in \textit{addEntryToSTable}, we use our new key K'. Then we add the new mapping (K, K') to \textit{M}. This keeps track of key K' and its concolic values, for when we need to do a query on the symbol table.

\newpage
\subsubsection{After Function Execution}
When a function is done executing, the entries in the symbol table for that particular version of function are deleted. To do this, we again use the hashmap to get the current keys (containing current version of variables also) of symbol table that are no longer required.

\subsection{Bound on Recursion}
To keep the size of symbol table in check and prevent the variable stack from overflowing, we have to put an upper limit on the maximum number of instances that the variable stack can have of a given function. This puts a cap on the precision of symbolic execution. \\
If the maximum number of versions allowed for a particular function be \textit{C}, then the symbolic execution will only continue uptil stack depth of C, after that the function executes using only concrete values. The symbolic execution resumes when the stack depth with respect to that function becomes less than or equal to \textit{C}.









\chapter{Experiments}
This chapter states the results generated by the tool on a certain set of programs. These results are then compared with intraprocedural concolic testing. 
\section{System Specifications}
\begin{table}[h]
\centering
\begin{tabular}{||l || l ||} 
 \hline
  \textbf{Operating System} & Ubuntu 14.04 LTS  \\ [1ex]
 \hline
   \textbf{OS Type} & 64-bit  \\[1ex]
 \hline
   \textbf{Processor} & Intel Core i7-4710HQ CPU @ 2.50 GHz $\times$ 8  \\ [1ex] 
  \hline
  \textbf{Memory} & 7.7 GiB, 1600 MHz  \\[1ex]
  \hline

\end{tabular}
\end{table}

\section{Result}
Interprocedural approach shows improvements where there are functions calls involved. When a function summaries its effect on the inputs variables, we get more precise path conditions. This is because instead of functions returning just concrete values they return symbolic values as well. These path conditions act as prepositional formula for test case generation. 
\\

Table 4.1 shows the results generated on some benchmark programs. The time recorded includes the time taken to complete the instrumentation and runtime of test case generation for a program. Instrumentating the code takes about 250 milliseconds for each program. The interprocedural analysis causes a slowdown of maximum 10 milliseconds in the test case generation. The maximum time recorded to run the tool on a program is just under 1.5 seconds.
\\

Interprocedural implementation shows good improvements in TCAS benchmark and some others. In shows coverage improvement from \textbf{33\%} to \textbf{83\%} in TCAS-2. It achieves this with a reduced number of test cases.  

\vspace{0.5 cm}
\begin{table}[hbtp]
\centering
\footnotesize
%%\hspace{-7 em}
\resizebox{\textwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
\textbf{S NO}            & \textbf{Program}            & \multicolumn{3}{c|}{\textbf{Intraprocedural Tesing}}			& \multicolumn{3}{c|}{\textbf{Interprocedural Testing}}                   \\ \hline
\textbf{}                & \textbf{}                         & \textbf{Coverage} & \textbf{\#TestCases}			 & \textbf{Time} & \textbf{Coverage} & \textbf{\#TestCases} & \textbf{Time} \\ \hline
1                        & primemod                          & 66                & 2                            & 1.010 sec  & 100		& 4			& 1.219 sec   \\ \hline
2                        & TCAS-1             & 14                & 11                            & 1.009 sec  & 85 		& 9 		& 1.240 sec  \\ \hline
3                        & TCAS-2                   & 33                & 8                            & 0.998 sec   & 83		& 7		 & 1.289 sec \\ \hline
4                        & heapSort                          & 100                & 2                            & 0.900 sec  & 100		& 2			& 1.143 sec    \\ \hline
5                        & crest1                          & 50                & 1                            & 0.915 sec  & 100		& 2			& 1.108 sec   \\ \hline
6                        & crest2             & 50                & 1                            & 1.048 sec  & 50		& 1		& 1.191 sec   \\ \hline
7                        & prime                    & 83                & 3                            & 1.029 sec   & 50		& 2			& 1.243 sec  \\ \hline
8                        & spectral-norm                          & 75                & 5              			& 1.304 sec  & 75			& 5			& 1.412 sec   \\ \hline
9		   & linpack		& 84		& 43		& 1.185			&52*			& 6*			& 1.395 sec*
\\ \hline
10			& faculty		& 50		& 3			& 0.853 sec 	& 100			& 2				& 1.160 sec
\\ \hline 
11			& totinfo		& 		& 			&  sec 	& 			& 				& 
\\ \hline 


\end{tabular}}
\caption{Benchmark Results}
\end{table}


All the programs have function calls but not all of them show improvement in coverage. The reason is that to influence the coverage, the functions should effect the path conditions. To do this, functions should alter the symbolic state of the variables of the program. That is why programs like \textbf{crest2} have several function calls but show no improvement in coverage. 

A WCET benchmark program \textbf{faculty} highlights the handling of recursive features. This program contains a functions that calculates factorial recusively. The intraprocedural testing gives 50\% coverage in 3 test cases whereas interprocedural testing improves it to 100 \% in 2 test cases.  

In concolic execution test cases are generated according to the path constraint obtained in the consecutive runs. The last condition of path constraint is flipped and passed to constraint solver to obtain new test case that explores a different path. But concolic engine we are using also aims at minimizing the number of test cases. Therefore flipping of conditions is done on the basis of levels. The conditions on the same level are flipped at the same time.\\ 

\vspace{-5mm}
\hspace{5cm}
\textbf{if} (C1)   \\

\vspace{-10mm}
\hspace{5.25cm} statement1;
\hspace{6cm}\\ 

\vspace{-8mm}
\hspace{5cm}
\textbf{if} (C2)   \\

\vspace{-10mm}
\hspace{5.25cm} statement2;
\hspace{6cm}  \\

If there are two conditions \textit{C1} and \textit{C2} that are on the same level then they will be flipped together to get the new test case. If execution of \textit{C2} block depends on the execution of \textit{statement1} then the coverage achieved may not be maximum. This is what happens in WCET program \textbf{prime} where the intraprocedural testing gives 83\% coverage while interprocedural testing gives 50\% coverage.


\section{Analysis}













\chapter{Conclusion}
\newpage

\addcontentsline{toc}{chapter}{Bibliography}
\begin{center}
\textbf{\Large{Bibliography}}
\end{center}


\newpage
\vspace{0.5cm} 
\begin{tabular}{||c c c||} 
 \hline
 \textbf{key} & \textbf{symVal} & \textbf{conVal} \\ [0.5ex] 
 \hline\hline
   & : &   \\ [1ex]
  \hline
  fac\_n\_1 & s1 & c1 \\ [-1ex]
  fac\_m\_1 & - & - \\
  \hline
   & : &  \\ [1ex] 
  \hline
  fac\_n\_0 & s0 & c0 \\[-1ex]
  fac\_m\_1 & - & - \\
  \hline
  & : &  \\ [1ex] 
  \hline
  main\_i\_0 & s' & c' \\[-1ex]
  main\_j\_0 & - & - \\
  \hline
  & : &  \\ [1ex] 
 \hline
\end{tabular}
 


\end{document}
